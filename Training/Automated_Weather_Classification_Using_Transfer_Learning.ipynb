{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efdb097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input \n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from PIL import ImageFile\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37dab151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator (rescale = 1./255,\n",
    "                                    shear_range = 0.2,\n",
    "                                    zoom_range= [.99, 1.01],\n",
    "                                    brightness_range= [0.8, 1.2],\n",
    "                                    data_format= \"channels_last\",\n",
    "                                    fill_mode=\"constant\",\n",
    "                                    horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d81fd30d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('dataset/train',\n",
    "                                                 target_size = (180, 180),\n",
    "                                                 batch_size = 64,\n",
    "                                                 class_mode = 'categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01463ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('dataset/test',\n",
    "                                            target_size = (180, 180),\n",
    "                                            batch_size = 64,\n",
    "                                            class_mode ='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68f29c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [180,180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf6c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG19 = VGG19(input_shape=IMAGE_SIZE + [3], weights=\"imagenet\",include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c39cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in VGG19.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = Flatten()(VGG19.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b049875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(5, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=VGG19.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cf39e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 180, 180, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 180, 180, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 90, 90, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 90, 90, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 90, 90, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 45, 45, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 45, 45, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 45, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 45, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 45, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 22, 22, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 22, 22, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 22, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 22, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 22, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 11, 11, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 5, 5, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 64005     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,088,389\n",
      "Trainable params: 64,005\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2c49126",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"categorical_crossentropy\",optimizer = \"adam\",metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540eed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "24/24 [==============================] - 474s 20s/step - loss: 1.1474 - accuracy: 0.5607 - val_loss: 1.2141 - val_accuracy: 0.7407\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 476s 20s/step - loss: 0.5603 - accuracy: 0.8127 - val_loss: 1.2654 - val_accuracy: 0.7407\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 376s 15s/step - loss: 0.4323 - accuracy: 0.8493 - val_loss: 1.1858 - val_accuracy: 0.7407\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 340s 14s/step - loss: 0.3579 - accuracy: 0.8807 - val_loss: 1.3781 - val_accuracy: 0.7407\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 386s 16s/step - loss: 0.3236 - accuracy: 0.9020 - val_loss: 1.2031 - val_accuracy: 0.7778\n",
      "Epoch 6/50\n",
      "24/24 [==============================] - 416s 17s/step - loss: 0.2709 - accuracy: 0.9313 - val_loss: 1.3187 - val_accuracy: 0.7778\n",
      "Epoch 7/50\n",
      "24/24 [==============================] - 388s 16s/step - loss: 0.2532 - accuracy: 0.9300 - val_loss: 1.2646 - val_accuracy: 0.7778\n",
      "Epoch 8/50\n",
      "24/24 [==============================] - 361s 15s/step - loss: 0.2363 - accuracy: 0.9353 - val_loss: 1.4382 - val_accuracy: 0.7407\n",
      "Epoch 9/50\n",
      "24/24 [==============================] - 397s 17s/step - loss: 0.2063 - accuracy: 0.9393 - val_loss: 1.5990 - val_accuracy: 0.7407\n",
      "Epoch 10/50\n",
      "24/24 [==============================] - 366s 15s/step - loss: 0.1948 - accuracy: 0.9500 - val_loss: 1.5201 - val_accuracy: 0.7407\n",
      "Epoch 11/50\n",
      "24/24 [==============================] - 211s 9s/step - loss: 0.1859 - accuracy: 0.9527 - val_loss: 1.4765 - val_accuracy: 0.7407\n",
      "Epoch 12/50\n",
      "24/24 [==============================] - 195s 8s/step - loss: 0.1622 - accuracy: 0.9587 - val_loss: 1.6787 - val_accuracy: 0.7407\n",
      "Epoch 13/50\n",
      "24/24 [==============================] - 185s 8s/step - loss: 0.1600 - accuracy: 0.9580 - val_loss: 1.6563 - val_accuracy: 0.7407\n",
      "Epoch 14/50\n",
      "24/24 [==============================] - 218s 9s/step - loss: 0.1372 - accuracy: 0.9673 - val_loss: 1.6121 - val_accuracy: 0.7407\n",
      "Epoch 15/50\n",
      "24/24 [==============================] - 220s 9s/step - loss: 0.1378 - accuracy: 0.9707 - val_loss: 1.7971 - val_accuracy: 0.7778\n",
      "Epoch 16/50\n",
      "24/24 [==============================] - 218s 9s/step - loss: 0.1284 - accuracy: 0.9780 - val_loss: 1.6361 - val_accuracy: 0.7407\n",
      "Epoch 17/50\n",
      "24/24 [==============================] - 218s 9s/step - loss: 0.1262 - accuracy: 0.9733 - val_loss: 1.6638 - val_accuracy: 0.7407\n",
      "Epoch 18/50\n",
      "24/24 [==============================] - 219s 9s/step - loss: 0.1251 - accuracy: 0.9693 - val_loss: 1.7405 - val_accuracy: 0.7037\n",
      "Epoch 19/50\n",
      "24/24 [==============================] - 222s 9s/step - loss: 0.1115 - accuracy: 0.9753 - val_loss: 1.8661 - val_accuracy: 0.7407\n",
      "Epoch 20/50\n",
      "24/24 [==============================] - 231s 10s/step - loss: 0.1051 - accuracy: 0.9787 - val_loss: 1.8828 - val_accuracy: 0.7407\n",
      "Epoch 21/50\n",
      "24/24 [==============================] - 220s 9s/step - loss: 0.1044 - accuracy: 0.9767 - val_loss: 1.7685 - val_accuracy: 0.7407\n",
      "Epoch 22/50\n",
      "24/24 [==============================] - 227s 9s/step - loss: 0.0942 - accuracy: 0.9807 - val_loss: 1.7204 - val_accuracy: 0.7037\n",
      "Epoch 23/50\n",
      "24/24 [==============================] - 228s 9s/step - loss: 0.0988 - accuracy: 0.9793 - val_loss: 1.9246 - val_accuracy: 0.7407\n",
      "Epoch 24/50\n",
      "24/24 [==============================] - 241s 10s/step - loss: 0.0823 - accuracy: 0.9867 - val_loss: 2.0366 - val_accuracy: 0.7407\n",
      "Epoch 25/50\n",
      "24/24 [==============================] - 234s 10s/step - loss: 0.0871 - accuracy: 0.9827 - val_loss: 1.8802 - val_accuracy: 0.7407\n",
      "Epoch 26/50\n",
      "24/24 [==============================] - 245s 10s/step - loss: 0.1012 - accuracy: 0.9753 - val_loss: 2.1191 - val_accuracy: 0.7407\n",
      "Epoch 27/50\n",
      "24/24 [==============================] - 241s 10s/step - loss: 0.0878 - accuracy: 0.9807 - val_loss: 1.9757 - val_accuracy: 0.7778\n",
      "Epoch 28/50\n",
      "24/24 [==============================] - 216s 9s/step - loss: 0.0817 - accuracy: 0.9840 - val_loss: 1.7851 - val_accuracy: 0.7778\n",
      "Epoch 29/50\n",
      "24/24 [==============================] - 222s 9s/step - loss: 0.0833 - accuracy: 0.9787 - val_loss: 2.2527 - val_accuracy: 0.7407\n",
      "Epoch 30/50\n",
      "24/24 [==============================] - 215s 9s/step - loss: 0.0670 - accuracy: 0.9873 - val_loss: 1.9496 - val_accuracy: 0.7407\n",
      "Epoch 31/50\n",
      "20/24 [========================>.....] - ETA: 37s - loss: 0.0703 - accuracy: 0.9883"
     ]
    }
   ],
   "source": [
    "model.fit(x = training_set, validation_data = test_set, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_set,\n",
    "                                   steps=11,\n",
    "                                   verbose=2,\n",
    "                                   use_multiprocessing=True,\n",
    "                                   workers=2)\n",
    "print(f\"Model performance on test images:\\nAccuracy = {accuracy}\\nLoss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f393ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"wcv.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d96a2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model \n",
    "from tensorflow.keras.preprocessing import image \n",
    "import numpy as np\n",
    "\n",
    "model = load_model(\"wcv.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "289fc705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 142ms/step\n",
      "rainy\n"
     ]
    }
   ],
   "source": [
    "img = image.load_img(r\"OIP (4).jpeg\",target_size= (180,180))\n",
    "\n",
    "x = image.img_to_array(img)#image to array \n",
    "x = np.expand_dims (x,axis=0) #changing the shape\n",
    "preds = model.predict(x)\n",
    "pred = np.argmax(preds, axis=1)\n",
    "index = ['cloudy', 'foggy', 'rainy', 'shine', 'sunrise']\n",
    "result = str(index[pred[0]])\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
